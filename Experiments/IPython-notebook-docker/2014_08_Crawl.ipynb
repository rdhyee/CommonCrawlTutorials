{
 "metadata": {
  "name": "",
  "signature": "sha256:7ddab8cd6580d537b4ec4f006097a6ac1dc8ca031a3548fc9e886e0741e1d5c3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boto\n",
      "from boto.s3.connection import S3Connection\n",
      "from boto.s3.key import Key\n",
      "\n",
      "from itertools import islice\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I don't like this having to pass in a bucket to keys_in_bucket.  Let's rewrite to pass in bucket_name\n",
      "\n",
      "def keys_in_bucket(prefix, bucket_name=\"aws-publicdatasets\", truncate_path=True, limit=None):\n",
      "    \"\"\"\n",
      "    given a S3 boto bucket, yields the keys in bucket with the prefix\n",
      "    if truncate_path is True, remove prefix in keys\n",
      "    optional limit to number of keys to return\n",
      "    \"\"\"\n",
      "    \n",
      "    conn = S3Connection(anon=True)\n",
      "    bucket = conn.get_bucket('aws-publicdatasets')\n",
      "    \n",
      "    keys = islice(bucket.list(prefix=prefix, \n",
      "                                   delimiter=\"/\"),\n",
      "                       limit)\n",
      "    for key in keys:\n",
      "        name = key.name.encode(\"UTF-8\") \n",
      "        if truncate_path:\n",
      "            name = name.replace(prefix, \"\", 1)\n",
      "        yield name\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def S3Key(key_name, bucket_name=\"aws-publicdatasets\"):\n",
      "    conn = S3Connection(anon=True)\n",
      "    bucket = conn.get_bucket(bucket_name)\n",
      "    return bucket.get_key(key_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-data/ | grep CC-MAIN"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# August 2014 crawl\n",
      "\n",
      "http://commoncrawl.org/august-2014-crawl-data-available/\n",
      "\n",
      "* all segments (CC-MAIN-2014-35/segment.paths.gz)\n",
      "* all WARC files (CC-MAIN-2014-35/warc.paths.gz)\n",
      "* all WAT files (CC-MAIN-2014-35/wat.paths.gz)\n",
      "* all WET files (CC-MAIN-2014-35/wet.paths.gz)\n",
      "\n",
      "https://aws-publicdatasets.s3.amazonaws.com/common-crawl/crawl-data/CC-MAIN-2014-35/segment.path.gz\n",
      "\n",
      "or \n",
      "\n",
      "https://aws-publicdatasets.s3.amazonaws.com/common-crawl/crawl-data/CC-MAIN-2014-35/segment.paths.gz ?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def key_for_new_crawl(crawl_name, file_type='all', return_name=False, plural=True):\n",
      "    \n",
      "    suffix = \"s\" if plural else \"\"\n",
      "    \n",
      "    if file_type.upper() == 'WARC':\n",
      "        file_name = \"warc.path{suffix}.gz\".format(suffix=suffix)\n",
      "    elif file_type.upper() == 'WAT':\n",
      "        file_name = \"wat.path{suffix}.gz\".format(suffix=suffix)\n",
      "    elif file_type.upper() == 'wet':\n",
      "        file_name = \"wet.path{suffix}.gz\".format(suffix=suffix)\n",
      "    else:\n",
      "        file_name = \"segment.path{suffix}.gz\".format(suffix=suffix)\n",
      "        \n",
      "\n",
      "    key_name = \"common-crawl/crawl-data/{crawl_name}/{file_name}\".format(crawl_name=crawl_name,\n",
      "                                                                       file_name=file_name)\n",
      "    \n",
      "    return S3Key(key_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(keys_in_bucket(\"common-crawl/crawl-data/CC-MAIN-2014-15/\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = S3Key(\"common-crawl/crawl-data/CC-MAIN-2014-35/segment.path.gz\")\n",
      "k"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k.size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = key_for_new_crawl(\"CC-MAIN-2014-35\", \"all\", plural=False)\n",
      "k"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do all of the nutch era crawls follow the same structure?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[key for key in keys_in_bucket(\"common-crawl/crawl-data/\") if key.startswith(\"CC-MAIN\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert year/week number to datetime\n",
      "\n",
      "import datetime\n",
      "\n",
      "def week_in_year(year, week):\n",
      "    return datetime.datetime(year,1,1) + datetime.timedelta((week-1)*7)\n",
      "\n",
      "week_in_year(2014,35)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip\n",
      "import StringIO\n",
      "\n",
      "# the following functions \n",
      "\n",
      "def gzip_from_key(key_name, bucket_name=\"aws-publicdatasets\"):\n",
      "    \n",
      "    k = S3Key(key_name, bucket_name)\n",
      "    f = gzip.GzipFile(fileobj=StringIO.StringIO(k.get_contents_as_string()))\n",
      "    return f\n",
      "\n",
      "def segments_from_gzip(gz):\n",
      "    s = gz.read()\n",
      "    return filter(None, s.split(\"\\n\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's parse segment.path(s).gz\n",
      "\n",
      "valid_segments = segments_from_gzip(gzip_from_key(key_for_new_crawl(\"CC-MAIN-2014-35\", \"all\", plural=False, return_name=True)))\n",
      "valid_segments[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rewrite to deal with gzip on the fly?\n",
      "\n",
      "from boto.s3.key import Key\n",
      "from gzipstream import GzipStreamFile\n",
      "\n",
      "def gzip_from_key_2(key_name, bucket_name='aws-publicdatasets'):\n",
      "\n",
      "    k = S3Key(key_name, bucket_name)\n",
      "    f = GzipStreamFile(k)\n",
      "    \n",
      "    return f\n",
      "\n",
      "def segments_from_gzip_2(gz):\n",
      "    BUF_SIZE = 1000\n",
      "    s = \"\"\n",
      "    buffer = gz.read(BUF_SIZE)\n",
      "    while buffer:\n",
      "        s += buffer\n",
      "        buffer = gz.read(BUF_SIZE)\n",
      "\n",
      "    return filter(None, s.split(\"\\n\"))\n",
      "\n",
      "valid_segments_2 = segments_from_gzip_2(gzip_from_key_2(\"common-crawl/crawl-data/CC-MAIN-2014-15/segment.paths.gz\"))\n",
      "valid_segments_2[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "WAT:  metadata files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "all WAT files (CC-MAIN-2014-15/wat.paths.gz)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments = segments_from_gzip(gzip_from_key(key_for_new_crawl(\"CC-MAIN-2014-35\", \"WAT\", plural=False, return_name=True)))\n",
      "len(wat_segments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://commoncrawl.org/navigating-the-warc-file-format/\n",
      "\n",
      "json file\n",
      "\n",
      "forgot to check how big the file is before downloading it.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = S3Key(wat_segments[0])\n",
      "k.size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_key_sizes(keys, bucket_name=\"aws-publicdatasets\"):\n",
      "    try:\n",
      "        conn = S3Connection(anon=True)\n",
      "        bucket = conn.get_bucket(bucket_name)\n",
      "\n",
      "        sizes = []\n",
      "        for key in keys:\n",
      "            try:\n",
      "                k = bucket.get_key(key)\n",
      "                sizes.append((key, k.size if hasattr(k, 'size') else 0))\n",
      "            except Exception, e:\n",
      "                sizes.append((key, e))\n",
      "        return sizes\n",
      "    except Exception, e:\n",
      "        sizes = []\n",
      "        for key in keys:\n",
      "            sizes.append ((key, e))\n",
      "        return sizes\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_key_sizes(wat_segments[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://stackoverflow.com/questions/2348317/how-to-write-a-pager-for-python-iterators/2350904#2350904        \n",
      "def grouper(iterable, page_size):\n",
      "    page= []\n",
      "    for item in iterable:\n",
      "        page.append( item )\n",
      "        if len(page) == page_size:\n",
      "            yield page\n",
      "            page= []\n",
      "    if len(page) > 0:\n",
      "        yield page\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "multiprocessing approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "when to use Processes vs Threads?  I think you use threads when there's no danger of memory collision\n",
      "\n",
      "http://sebastianraschka.com/Articles/2014_multiprocessing_intro.html#Multi-Threading-vs.-Multi-Processing:\n",
      "\n",
      ">  If we submit \"jobs\" to different threads, those jobs can be pictured as \"sub-tasks\" of a single process and those threads will usually have access to the same memory areas (i.e., shared memory). This approach can easily lead to conflicts in case of improper synchronization, for example, if processes are writing to the same memory location at the same time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "multiprocessing async approach"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# with pool.map\n",
      "\n",
      "from multiprocessing.dummy import Pool as ThreadPool \n",
      "from functools import partial\n",
      "\n",
      "get_key_sizes_for_bucket = partial(get_key_sizes, bucket_name=\"aws-publicdatasets\")\n",
      "\n",
      "PAGE_SIZE = 1\n",
      "POOL_SIZE = 100\n",
      "MAX_SEGMENTS = 100\n",
      "\n",
      "pool = ThreadPool(POOL_SIZE) \n",
      "results = pool.map(get_key_sizes_for_bucket, grouper(wat_segments[0:MAX_SEGMENTS], PAGE_SIZE))\n",
      "pool.close()\n",
      "pool.join()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pool.imap_unordered\n",
      "\n",
      "from multiprocessing.dummy import Pool as ThreadPool \n",
      "from functools import partial\n",
      "\n",
      "get_key_sizes_for_bucket = partial(get_key_sizes, bucket_name=\"aws-publicdatasets\")\n",
      "\n",
      "PAGE_SIZE = 1\n",
      "POOL_SIZE = 100\n",
      "MAX_SEGMENTS = 1000  # replace with None for all segments\n",
      "CHUNK_SIZE = 10\n",
      "\n",
      "pool = ThreadPool(POOL_SIZE) \n",
      "results_iter = pool.imap_unordered(get_key_sizes_for_bucket, \n",
      "                              grouper(wat_segments[0:MAX_SEGMENTS], PAGE_SIZE),\n",
      "                              CHUNK_SIZE)\n",
      "\n",
      "results = []\n",
      "                             \n",
      "for (i, page) in enumerate(islice(results_iter,None)):\n",
      "    print ('\\r>> Result %d' % i, end=\"\")\n",
      "    for result in page:\n",
      "        results.append(result)\n",
      "            \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pool.imap_unordered with ProcessPool\n",
      "# not necessarily that useful since this calculation is I/O bound\n",
      "\n",
      "from multiprocessing import Pool as ProcessPool \n",
      "from functools import partial\n",
      "\n",
      "get_key_sizes_for_bucket = partial(get_key_sizes, bucket_name=\"aws-publicdatasets\")\n",
      "\n",
      "PAGE_SIZE = 1\n",
      "POOL_SIZE = 100\n",
      "MAX_SEGMENTS = 500  # replace with None for all segments\n",
      "CHUNK_SIZE = 10\n",
      "\n",
      "pool = ProcessPool(POOL_SIZE) \n",
      "results_iter = pool.imap_unordered(get_key_sizes_for_bucket, \n",
      "                              grouper(wat_segments[0:MAX_SEGMENTS], PAGE_SIZE),\n",
      "                              CHUNK_SIZE)\n",
      "\n",
      "results = []\n",
      "                             \n",
      "for (i, page) in enumerate(islice(results_iter,None)):\n",
      "    print ('\\r>> Result %d' % i, end=\"\")\n",
      "    for result in page:\n",
      "        results.append(result)\n",
      "            \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import Series, DataFrame\n",
      "df = DataFrame(results, columns=['key', 'size'])\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['size'].describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as P\n",
      "P.hist(df['size'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# total byte size of all the wat files\n",
      "print (format(sum(df['size']),\",d\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save results\n",
      "import csv\n",
      "with open('CC-MAIN-2014-35.wat.csv', 'wb') as csvfile:\n",
      "    wat_writer = csv.writer(csvfile, delimiter=',',\n",
      "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
      "    wat_writer.writerow(['key', 'size'])\n",
      "    for result in results:\n",
      "        wat_writer.writerow(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head CC-MAIN-2014-35.wat.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Taking apart a WAT file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this won't work for the version of boto I'm currently using -- a bug to be resolved\n",
      "\n",
      "# key = bucket.get_key(wat_segments[0])\n",
      "# url = key.generate_url(expires_in=0, query_auth=False)\n",
      "# url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# looks like within any segment ID, we should expect warc, wat, wet buckets\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-35/segments/1408500800168.29/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#https://github.com/commoncrawl/gzipstream\n",
      "\n",
      "from gzipstream import GzipStreamFile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boto\n",
      "from itertools import islice\n",
      "from boto.s3.key import Key\n",
      "from gzipstream import GzipStreamFile\n",
      "import warc\n",
      "import json\n",
      "\n",
      "def test_gzipstream():\n",
      "\n",
      "  output = []\n",
      "    \n",
      "  # Let's use a random gzipped web archive (WARC) file from the 2014-15 Common Crawl dataset\n",
      "  ## Connect to Amazon S3 using anonymous credentials\n",
      "  conn = boto.connect_s3(anon=True)\n",
      "  pds = conn.get_bucket('aws-publicdatasets')\n",
      "  ## Start a connection to one of the WARC files\n",
      "  k = Key(pds)\n",
      "  k.key = 'common-crawl/crawl-data/CC-MAIN-2014-15/segments/1397609521512.15/warc/CC-MAIN-20140416005201-00000-ip-10-147-4-33.ec2.internal.warc.gz'\n",
      "\n",
      "  # The warc library accepts file like objects, so let's use GzipStreamFile\n",
      "  f = warc.WARCFile(fileobj=GzipStreamFile(k))\n",
      "  for num, record in islice(enumerate(f),100):\n",
      "    if record['WARC-Type'] == 'response':\n",
      "      # Imagine we're interested in the URL, the length of content, and any Content-Type strings in there\n",
      "      output.append((record['WARC-Target-URI'], record['Content-Length']))\n",
      "      output.append( '\\n'.join(x for x in record.payload.read().replace('\\r', '').split('\\n\\n')[0].split('\\n') if 'content-type:' in x.lower()))\n",
      "      output.append( '=-=-' * 10)\n",
      " \n",
      "  return output\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test_gzipstream()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def warc_records(key_name, limit=None):\n",
      "    conn = boto.connect_s3(anon=True)\n",
      "    bucket = conn.get_bucket('aws-publicdatasets')\n",
      "    key = bucket.get_key(key_name)\n",
      "\n",
      "    # The warc library accepts file like objects, so let's use GzipStreamFile\n",
      "    f = warc.WARCFile(fileobj=GzipStreamFile(key))\n",
      "    for record in islice(f, limit):\n",
      "        yield record"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's compute some stats on the headers\n",
      "from collections import Counter\n",
      "c=Counter()\n",
      "[c.update(record.header.keys()) for record in warc_records(wat_segments[0],1)]\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# warc-type\n",
      "\n",
      "# looks like \n",
      "# first record is 'warcinfo\n",
      "# rest is 'metadata'\n",
      "\n",
      "c=Counter()\n",
      "[c.update([record['warc-type']]) for record in warc_records(wat_segments[0],100)]\n",
      "\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# content-type\n",
      "\n",
      "c=Counter()\n",
      "[c.update([record['content-type']]) for record in warc_records(wat_segments[0],100)]\n",
      "\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what's in the records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wrecords = warc_records(wat_segments[0])\n",
      "wrecords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record = wrecords.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if record.header.get('content-type') == 'application/json':\n",
      "    s = record.payload.read()\n",
      "    payload = json.loads(s)\n",
      "else:\n",
      "    payload = s\n",
      "    \n",
      "payload"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record.header.get('content-type')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record.url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://warc.readthedocs.org/en/latest/#working-with-warc-header\n",
      "\n",
      "record.header.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#payload\n",
      "dir(record.payload)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record.header.get('content-type')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(payload)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if isinstance(payload, dict):\n",
      "    print (payload['Envelope']['WARC-Header-Metadata'].get('WARC-Target-URI'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record.header"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse\n",
      "urlparse.urlparse(payload['Envelope']['WARC-Header-Metadata']['WARC-Target-URI']).netloc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's just run through a segment with minimal processing (to test processing speed)\n",
      "\n",
      "import urlparse\n",
      "\n",
      "def walk_through_segment(segment_id, limit=None):\n",
      "    \n",
      "    for (i,record) in enumerate(warc_records(segment_id,limit)):\n",
      "        print (\"\\r>> Record: %d\" % i, end=\"\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time walk_through_segment(wat_segments[0],100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse\n",
      "\n",
      "def netloc_count_for_segment(segment_id, limit=None):\n",
      "    \n",
      "    c = Counter()\n",
      "\n",
      "    for (i, record) in enumerate(warc_records(segment_id,limit)):\n",
      "\n",
      "        print (\"\\r>>Record: %d\" % i, end=\"\")\n",
      "\n",
      "        if record.header.get('content-type') == 'application/json':\n",
      "            s = record.payload.read()\n",
      "            payload = json.loads(s)\n",
      "            url = payload['Envelope']['WARC-Header-Metadata'].get('WARC-Target-URI')\n",
      "            if url:\n",
      "                netloc = urlparse.urlparse(url).netloc\n",
      "                c.update([netloc])\n",
      "            else:\n",
      "                c.update([None])\n",
      "\n",
      "    return c\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time netloc_count_for_segment(wat_segments[0],100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rewrite dropping Counter to use defaultdict\n",
      "# http://evanmuehlhausen.com/simple-counters-in-python-with-benchmarks/ suggests Counter is slow\n",
      "\n",
      "import urlparse\n",
      "from collections import defaultdict\n",
      "\n",
      "counter = defaultdict(int)\n",
      "\n",
      "def netloc_count_for_segment_dd(segment_id, limit=None):\n",
      "\n",
      "    c = defaultdict(int)\n",
      "    \n",
      "    for (i, record) in enumerate(warc_records(segment_id,limit)):\n",
      "\n",
      "        print (\"\\r>>Record: %d\" % i, end=\"\")\n",
      "        s = record.payload.read()\n",
      "        if record.header.get('content-type') == 'application/json':\n",
      "            payload = json.loads(s)\n",
      "            url = payload['Envelope']['WARC-Header-Metadata'].get('WARC-Target-URI')\n",
      "            if url:\n",
      "                netloc = urlparse.urlparse(url).netloc\n",
      "                c[netloc] += 1\n",
      "            else:\n",
      "                c[None] += 1\n",
      "\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time netloc_count_for_segment_dd(wat_segments[0],1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Tag Count"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# draw from https://github.com/Smerity/cc-mrjob/blob/7ab5a81ee698a2819ae1bc5295ac0de628f1ea6a/mrcc.py\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-35/warc.path.gz"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# WARC files from the latest crawl\n",
      "# s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-35/\n",
      "\n",
      "warc_segments = segments_from_gzip(gzip_from_key(key_for_new_crawl(\"CC-MAIN-2014-23\", \"WARC\", return_name=True, plural=False)))\n",
      "len(warc_segments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "warc_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "walk_through_segment(warc_segments[0],100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# code adapted from https://github.com/Smerity/cc-mrjob/blob/7ab5a81ee698a2819ae1bc5295ac0de628f1ea6a/tag_counter.py\n",
      "\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "# Optimization: compile the regular expression once so it's not done each time\n",
      "# The regular expression looks for (1) a tag name using letters (assumes lowercased input) and numbers\n",
      "# and (2) allows an optional for a space and then extra parameters, eventually ended by a closing >\n",
      "\n",
      "HTML_TAG_PATTERN = re.compile('<([a-z0-9]+)[^>]*>')\n",
      "\n",
      "def get_tag_count(data, ctr=None):\n",
      "  \"\"\"Extract the names and total usage count of all the opening HTML tags in the document\"\"\"\n",
      "  if ctr is None:\n",
      "    ctr = Counter()\n",
      "  # Convert the document to lower case as HTML tags are case insensitive\n",
      "  ctr.update(HTML_TAG_PATTERN.findall(data.lower()))\n",
      "  return ctr\n",
      "\n",
      "# Let's check to make sure the tag counter works as expected\n",
      "assert get_tag_count('<html><a href=\"...\"></a><h1 /><br/><p><p></p></p>') == {'html': 1, 'a': 1, 'p': 2, 'h1': 1, 'br': 1}\n",
      "\n",
      "\n",
      "def tag_counts_for_segment(segment_id, limit=None, print_record_num=False):\n",
      "    \n",
      "    ctr = Counter()\n",
      "    \n",
      "    try:\n",
      "    \n",
      "        for (i, record) in enumerate(warc_records(segment_id,limit)):\n",
      "\n",
      "            if print_record_num:\n",
      "                print (\"\\r>>Record: %d\" % i, end=\"\")\n",
      "\n",
      "            if record.header.get('content-type') == 'application/http; msgtype=response':\n",
      "                payload = record.payload.read()\n",
      "                # The HTTP response is defined by a specification: first part is headers (metadata)\n",
      "                # and then following two CRLFs (newlines) has the data for the response\n",
      "                headers, body = payload.split('\\r\\n\\r\\n', 1)\n",
      "                if 'Content-Type: text/html' in headers:\n",
      "                    # We avoid creating a new Counter for each page as that's actually quite slow\n",
      "                    tag_count = get_tag_count(body,ctr)\n",
      "  \n",
      "        return (segment_id, ctr)\n",
      "    \n",
      "    except Exception, e:\n",
      "        \n",
      "        return (segment_id, e)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's use multiprocessing to try to duplicate the list of segments on the mrjob example\n",
      "# https://raw.githubusercontent.com/commoncrawl/cc-mrjob/master/input/test-10.txt\n",
      "\n",
      "import requests\n",
      "mrjobs_segs = filter(None, \n",
      "                     requests.get(\"https://raw.githubusercontent.com/commoncrawl/cc-mrjob/master/input/test-100.txt\").content.split(\"\\n\"))\n",
      "mrjobs_segs "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from multiprocessing.dummy import Pool as ThreadPool \n",
      "from multiprocessing import Pool as ProcessPool\n",
      "\n",
      "from functools import partial\n",
      "\n",
      "\n",
      "def calc_tag_counts_for_segments(segments, max_segments=None, max_record_per_segment=None, pool_size=4, chunk_size=1,\n",
      "                                 print_result_count=False, threads=True):\n",
      "    \n",
      "    \"\"\"\n",
      "    pool_size:  number of works in pool\n",
      "    max_segments:  number of segments to calculate (None for no limit)\n",
      "    max_record_per_segment:  max num of records to calculate per segment (None for no limit)\n",
      "    chunk_size: how to chunk jobs among pool workers\n",
      "    print_result_count: whether to print number of results available\n",
      "    \"\"\"\n",
      "    \n",
      "    tag_counts_for_segment_limit = partial(tag_counts_for_segment, limit=max_record_per_segment)\n",
      "\n",
      "\n",
      "    if threads:\n",
      "        pool = ThreadPool(pool_size) \n",
      "    else:\n",
      "        pool = ProcessPool(pool_size)\n",
      "        \n",
      "    results_iter = pool.imap_unordered(tag_counts_for_segment_limit, \n",
      "                                  mrjobs_segs[0:max_segments],\n",
      "                                  chunk_size)\n",
      "\n",
      "    results = []\n",
      "    exceptions = []\n",
      "    net_counts = Counter()\n",
      "\n",
      "    for (i, (seg_id, result)) in enumerate(results_iter):\n",
      "        if print_result_count:\n",
      "            print ('\\r>> Result %d' % i, end=\"\")\n",
      "        results.append((seg_id, result))\n",
      "        if isinstance(result, Counter):\n",
      "            net_counts.update(result)\n",
      "        elif isinstance(result,Exception):\n",
      "            exceptions.append((seg_id, result))\n",
      "    \n",
      "    return (net_counts, results, exceptions)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time (tag_counts, results, exceptions) = calc_tag_counts_for_segments(mrjobs_segs, max_segments=None, \\\n",
      "                                                max_record_per_segment=100, \\\n",
      "                                                pool_size=8, \\\n",
      "                                                chunk_size=1, \\\n",
      "                                                print_result_count=True, \\\n",
      "                                                threads=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag_counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "exceptions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using IPython parallel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can have multithreading within a given process but to use multiple processes, need to go with IPython parallel -- at least, that's what I've been able to work out"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import multiprocessing\n",
      "multiprocessing.cpu_count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython import parallel\n",
      "rc = parallel.Client()\n",
      "rc.block = True\n",
      "\n",
      "rc.ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To be continued...."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "WARC files:  the final frontier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is it possible to work with pieces of the new warc files without having to download the whole file?  \n",
      "\n",
      "In the 2012 crawl data, there was an index built http://urlsearch.commoncrawl.org/ --> from which you can get a reference to an offset and length inside of the arc.gz file.  With S3, you don't need to then download the whole file but just a chunk....and that chunk itself is unpackable as a gzip file.  (nice feature of gzip?):\n",
      "\n",
      "http://nbviewer.ipython.org/github/rdhyee/working-open-data/blob/postscript/notebooks/Day_21_CommonCrawl_Content.ipynb#Grabbing-pieces-of-the-.arc.gz-files\n",
      "\n",
      "Is there a similar way of working with the 2014 crawl data?  That is, a way of reading off chunks of the warc file without grabbing entire warc files?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}